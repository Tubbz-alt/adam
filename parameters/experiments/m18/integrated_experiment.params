_includes:
   - "../../root.params"

experiment: "integrated_experiment_pegasus"
experiment_group_dir: '%adam_experiment_root%/%experiment%'
curriculum_repository_path: "%adam_experiment_root%/curriculum/"

# Consistent Params
num_pursuit_learners_active: 8

# Configuration Params - Prelim
include_attributes: False
include_relations: False
object_learner_type: "pursuit"
attribute_learner_type: "none"
relation_learner_type: "none"

# Configuration Params - Desired
#include_attributes: False
#include_relations: False
#object_learner_type: "pursuit"
#attribute_learner_type: "pursuit"
#relation_learner_type: "pursuit"

integrated_learners_experiment:
   num_samples: 1000
   sort_learner_descriptions_by_length: true
   num_pretty_descriptions: 5

   log_hypothesis_every_n_steps: 50

# Workflow Params
workflow_name: "integrated_experiment_pegasus"
workflow_directory: '%adam_experiment_root%/%experiment%'
site: 'saga'
namespace: 'saga'

backend: slurm
partition: ephemeral
num_cpus: 1
num_gpus: 0
memory: '4G'  # not sure how much is needed
job_time_in_minutes: 720  # We request 12 hours as this is the maximum allotment for Ephemeral
                          # By requesting this much we can easily just restart the pegasus workflow
                          # And continue from the last checkpointed time. This is very useful for
                          # pursuit. It may also be wise to log more than every 100 instances
                          # to decrease the number of 'lost' training instances that can occur
                          # If the process is killed or errors out.